<!DOCTYPE html>

<html lang="en">

<head>
  <title>Chapter 2: Linear Regression with One Feature | An Introduction to Artificial Intelligence and Machine Learning Algorithms</title>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link rel="shortcut icon" href="../../assets/images/favicon.png" type="image/x-icon">

  <link rel="stylesheet" href="../../assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="../../assets/css/series.css">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700,800,300" type="text/css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@100;200;300;400&family=Roboto:wght@300;400;500&display=swap" type="text/css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=PT+Serif:ital,wght@0,400;0,700;1,400;1,700&display=swap">

  <script>
    MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
      svg: { fontCache: 'global' }
    };
  </script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-90367614-1');
  </script>

  <script src="https://www.googletagmanager.com/gtag/js?id=UA-90367614-1" async></script>
  <script src="https://kit.fontawesome.com/ddc5710cc6.js"></script>
</head>

<body data-spy="scroll" data-target=".site-navbar-target" data-offset="300" data-aos-easing="slide" data-aos-duration="800" data-aos-delay="0" cz-shortcut-listen="true">
  <div class="sticky-wrapper" style="height: 95px;">
    <div class="site-navbar site-navbar-target">
        <div class="row align-items-center" style="margin-right: 0 !important;">
          <div class="col-12">
            <nav class="site-navigation text-right">
                <a class="home-icon-link" href="../../index.html" class="pl-3 pr-3"><span class="fas fa-home" style="color: white !important;"></span></a>
                <ul class="site-menu main-menu ">
                  <li><a href="https://twitter.com/BrandonJGrenier" class="pl-3 pr-3"><span class="fab fa-twitter"></span></a></li>
                  <li><a href="https://github.com/BrandonJohnGrenier" class="pl-3 pr-3"><span class="fab fa-github"></span></a></li>
                  <li><a href="https://linkedin.com/in/brandon-grenier" class="pl-3 pr-3"><span class="fab fa-linkedin-in"></span></a></li>
                </ul>
            </nav>
          </div>
        </div>
    </div>
  </div>

  <div class="site-section book-section">
    <div class="container">
      <div class="row">
        <div class="col-md-12 mb-3 mt-3 book-title">
          <h1 class="text-white text-uppercase mt-3 book-title-font">AN INTRODUCTION TO ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING ALGORITHMS</h1>
          <p class="text-white mb-5 text-uppercase" style="font-family: Open Sans !important; font-weight: 100 !important; font-size: 22px !important; opacity: 0.8;">Brandon John Grenier</p>
        </div>
      </div>
    </div>
  </div>

  <div class="site-section" style="background-color: #FFF; padding-top: 60px;">
    <div class="container chapter-container">
      <div class="row">
        <div class="col-md-12">

        <div class="row mb-5" style="margin-top: -25px;">
          <div class="col-md-6 text-right pr-2"><a href="chapter01.html">&laquo; Chapter 1</a> &nbsp;&bull;</div>
          <div class="col-md-6 text-left pl-0"><a href="chapter03.html">Chapter 3 &raquo;</a></div>
        </div>

        <h1 class="heading">Chapter 2: Linear Regression with One Feature</h1>

        <p>
          Linear regression is one of the simplest and most well understood algorithms in machine learning, and the
          first regression model algorithm that we’ll cover in detail. Regression models are used in supervised learning
          strategies to predict continuous values; in this chapter we’ll develop a linear regression algorithm to predict
          the price of real estate in Melbourne, Australia.
        </p>

        <p>
          As a practical introduction to linear regression, we’ll begin by making predictions <em>intuitively</em> – no math or algorithms
          will be involved, just a bit of guesswork. We’ll then formalize this intuition by introducing key machine learning concepts, mathematics
          and algorithms that underpin linear regression. By the end of this chapter, you will have all of the tools you need to build your own linear
          regression machine learning algorithm.
        </p>

        <p>
          As hinted by the chapter title, we’re going to build a linear regression algorithm that uses a single variable, or
          <em>feature</em>. Linear regression that uses a single variable is referred to as <strong>univariate linear regression</strong>.
          Due to its simplicity, univariate linear regression serves as a fantastic technical introduction to foundational machine learning
          concepts, models and mathematics. Although the machine learning algorithms in upcoming chapters will increase in
          complexity and sophistication, they effectively iterate on the foundations we introduce in this chapter.
        </p>




          <a name="sec-1"></a>
          <h2>Intuitive Linear Regression</h2>
          <p>
            The first prediction we’re going to make using linear regression will be an entirely intuitive one – there’s no math
            or algorithms involved at this point, just a bit of guesswork. As a consequence of this guesswork our predictions
            will be reasonable, and we’ll improve our predictive ability as we formalize our intuition.
          </p>

          <h3>Predicting the price of a home in Melbourne, Australia</h3>
          <p>
            Let’s assume that you would like to buy a 150 square meter home in Melbourne’s inner city suburb of Richmond.
            You’ve done some homework and collected data from twelve recent sales in the area - each data point consists of
            a home’s sale price and its size in square metres.
          </p>
          <p>
            Based on this data, you want to predict how much it might cost to buy a 150 square meter home in Richmond. For
            the sake of this exercise, we’ll assume that the price of a home is only dependent on a single <em>feature</em> - the
            size of the home.
          </p>
          <p>
            Let’s start with a simple method to figure out how much it might cost to buy a 150m<sup>2</sup> home in Richmond.
          </p>

          <h3>Step 1: Plot the data</h3>
          <p>
          As a first step, we’ll plot all of the sales data onto a graph. Figure 1 below presents
          a graph of the sales data, with house sizes (in m<sup>2</sup>) on the horizontal <em>x axis</em> and house
          prices on the vertical <em>y axis</em>.
          </p>

          <figure>Figure 1: House price as a function of house size</figure>
          <img src="images/chapter2/figure1.png"></img>

          <h3>Step 2: Fit the data</h3>
          <p>
            Now that our sales data is plotted on a graph, we can simply use our best judgement to draw a straight red line that fits
            the data reasonably well. Figure 2 below presents our red line overlaying the sales data.
          </p>

          <figure>Figure 2: Using judgement to draw a straight line that fits our housing data</figure>
          <img src="images/chapter2/figure2.png"></img>

          <h3>Step 3: Make a prediction</h3>
          <p>
            To predict the price of a 150m<sup>2</sup> home in Richmond, draw a vertical line from the 150m<sup>2</sup> value located on the x axis
            until it intersects with the red line we drew in step 2. From the intersection come across to the y axis to get
            the predicted price of a 150m<sup>2</sup> home. Figure 3 below provides you with a visual demonstration of our prediction using
            green dashed lines.
          </p>

          <figure>Figure 3: Predicting the price of a 150m<sup>2</sup> home in Richmond</figure>
          <img src="images/chapter2/figure3.png"></img>

          <p>
            Based on the sales data you collected, we can predict that a 150m<sup>2</sup> house in Richmond will cost
            approximately <strong>$900,000</strong> to purchase.
          </p>

          <p>
            Let’s quickly summarize the four steps we just took to make this prediction:
          </p>

          <ol>
            <li>We collected data</li>
            <li>We plotted the data</li>
            <li><em>We drew a straight line that best fit the data</em></li>
            <li>We used the line to make a prediction – an educated guess in the absence of data</li>
          </ol>

          <p>
            In step 3, we used our judgement to draw a straight line to fit the data as best as we could. For the remainder of
            this chapter, we’ll go on a journey to replace this <em>judgement</em> with <em>data-driven algorithms</em> which will
            ultimately perform the same function, but do so with far more accuracy.
          </p>


          <a name="sec-2"></a>
          <h2>The Hypothesis Function</h2>

          <p>
            The <strong>hypothesis function</strong> is a term that describes any mathematical function that is used to fit, or model
            your data. Problems spaces like population growth and infection rates may best be modelled with <em>exponential functions</em>;
            predicting yearly fluctuations in global temperature could potentially be modelled with <em>sinusoidal functions</em>;
            predicting earthquake behavior could be modelled with <em>logarithmic functions</em>.
          </p>

          <h3>The Hypothesis Function for Linear Regression</h3>

          <p>
            A <em>linear function</em> is optimal for modelling linear relationships like our home sales data, and will be the most
            appropriate linear regression hypothesis function for our current problem space, which only involves one variable - the
            size of a home. The linear regression hypothesis function is defined as:
          </p>

          <p><strong>h(x) = &Theta;<sub>0</sub> + &Theta;<sub>1</sub>x</strong></p>

          <p>
            The symbol &Theta; is called theta, you’ll see references to theta throughout machine learning.
          </p>

          <h3>The Goal of Linear Regression</h3>
          <p>
            Now that we’ve defined the hypothesis function for linear regression, we have enough context to define
            the <em>goal</em> of linear regression:
          </p>

          <blockquote>
            Given a training set, produce a hypothesis function h so that h(x) is a good predictor for y.
          </blockquote>

          <p>
            In other words, linear regression aims to produce a hypothesis function whose <em>predictions</em> match <em>observations</em>
            as closely as possible. It does this by identifying the best values for <strong>&Theta;<sub>0</sub></strong> and
            <strong>&Theta;<sub>1</sub></strong> so that the hypothesis function minimizes the difference between predictions and
            observations. We can concretely define a <strong>good predictor</strong> as a hypothesis function that most effectively <em>minimizes the difference</em> between predicted
            values <strong>h(x)</strong> and observed values <strong>y</strong>, for all values of <strong>x</strong> in a
            training set. The smaller the difference, the better the predictor; the larger the difference, the poorer the predictor.
          </p>

          <h4>Reader Tip: Linear Function Review</h4>
          <p>
            We’ve included a review of linear functions and a few visual examples of functions that are commonly used to model
            data in <a href="appendixA.html">Appendix A</a>. If it’s been a while since you’ve last used linear functions we
            recommend that you read Appendix A before you move on to the next section.
          </p>



          <a name="sec-3"></a>
          <h2>The Cost Function</h2>

          <p>
            The <strong>cost function</strong> measures how well a hypothesis function acts as a <em>good predictor</em> for a given
            training set, which we’ll refer to this as the <em>performance</em> of a hypothesis function going forward. It does this by
            computing the average difference between predicted values <strong>h(x)</strong> and observed values <strong>y</strong>, for all values
            of <strong>x</strong> in a training set. Let’s derive the cost function from first principles so that you can get an appreciation
            for its behavior and mechanics.
          </p>

          <h3>Deriving The Cost Function</h3>
          <br/>
          <p>
          <strong>Step 1:</strong>Calculate the difference between the predicted value h(x) and the observed value y, for each
          instance <strong>i</strong> in the training set.
          </p>

          $$h(x^{(i)}) - y^{(i)}$$

          <p>
          The superscript <strong>i</strong> in parentheses refers to a specific training <em>instance</em> in the training set.
          </p>

          <p><strong>Step 2:</strong> Sum up all of the differences for each instance in the training set.</p>

          $$\displaystyle \sum _{i=0}^m \left (h (x^{(i)}) - y^{(i)} \right)$$

          <p>
            The variable <strong>m</strong> refers to the size of the training set.
          </p>

          <p><strong>Step 3:</strong> Divide the total difference by the size of the training set to calculate the average difference for the training set.</p>

          $$\dfrac {1}{m} \displaystyle \sum _{i=0}^m \left (h (x^{(i)}) - y^{(i)} \right)$$

          <p>
            What we’ve just derived in step 3 is a perfectly usable cost function, but we’re going to make two
            modifications to simplify the implementation of the function in practice.
          </p>

          <p>
            First, we are going to square the error component (the difference between the predicted value <strong>h(x)</strong> and observed value <strong>y</strong>).
            Squaring always ensures that we have a positive number; for example 4<sup>2</sup> and -4<sup>2</sup> are both equal to 16. A
            programmatic implementation of the cost function only cares about the magnitude of error - getting rid of negatives removes a bit
            of complexity in dealing with both positive and negative numbers.
          </p>

          <p>
            Second, the entire cost function is multiplied by ½ so that the function remains consistent when derivatives of
            this function are taken. Since the derivative of x<sup2</sup> is 2x, multiplying by ½ negates the effect of the exponent.
          </p>

          <p>
            This is the formal definition of the cost function, which is also referred to as the squared error function or mean
            error function:
          </p>

          $$\dfrac {1}{2m} \displaystyle \sum _{i=0}^m \left (h (x^{(i)}) - y^{(i)} \right)^2$$

          <p>
            The cost function is conventionally defined as <strong>J</strong>. Lastly, we substitute the <em>reference</em> to our hypothesis function
            <strong>h(x<sub>i</sub>)</strong> with the linear regression hypothesis function <strong>h(x<sub>i</sub>) = &Theta;<sub>0</sub> + &Theta;<sub>1</sub>x<sub>i</sub></strong>.
            This is the fully expanded cost function that we will use in the linear regression algorithm:
          </p>

          $$J(\theta_0, \theta_1) = \dfrac {1}{2m} \displaystyle \sum _{i=0}^m \left (\theta_0 + \theta_1x^{(i)} - y^{(i)} \right)^2$$

          <p>
            We can now express the goal of linear regression in slightly more specific terms - the goal of linear regression is to identify values
            for Θ<sub>0</sub> and Θ<sub>1</sub> so that <strong>J(Θ<sub>0</sub>, Θ<sub>1</sub>)</strong> produces the smallest possible value. Put simply, the goal of linear
            regression is to <em>minimize</em> <strong>J(Θ<sub>0</sub>, Θ<sub>1</sub>)</strong>.
          </p>

          <h3>The Cost Function in Practice</h3>
          <p>
            To get a better appreciation of how the cost function works, we’ll run through an exercise to calculate and compare the
            cost function of two different linear regression hypothesis functions to find out which one would act as the
            better predictor.
          </p>

          <p>
            In figure 5 below, I’ve plotted the function <strong>h(x) = 3 + 11x</strong>, the function <strong>h(x) = 1 + 5x</strong> and a
            training set consisting of five data points. The data points in our training set are as follows:
          </p>

          <table>
            <thead><tr><td>x</td><td style="text-align: center;">y</td></tr></thead>
            <tr><td>5</td><td>30</td></tr>
            <tr><td>6</td><td>40</td></tr>
            <tr><td>7</td><td>50</td></tr>
            <tr><td>8</td><td>60</td></tr>
            <tr><td>9</td><td>70</td></tr>
          </table>
          <br/>

          <figure>Figure 5: Comparing the predictive performance of two linear regression hypothesis functions</figure>
          <img src="images/chapter2/figure5.png"></img>

          <p>
            Figure 5 suggests that that the function <strong>h(x) = 1 + 5x</strong> is a better fit for the data, and therefore a better
            predictor for this training set.  We’ll compute the cost for both functions and compare their performance to
            validate this intuition.
          </p>

          <p>
            Table 1 and Table 2 below show the work for calculating the cost function for each hypothesis function.
          </p>

          <figure>Table 1: Calculating the cost function for h(x) = 1 + 5x</figure>
          <table>
            <thead>
            <tr>
              <td>x</td>
              <td>Predicted Value</td>
              <td>Observed Value</td>
              <td>Difference</td>
              <td>Squared Difference</td>
            </tr>
          </thead>
            <tr>
              <td>5</td>
              <td>h(5) = 1 + 5*5 = 26</td>
              <td>30</td>
              <td>-4</td>
              <td>16</td>
            </tr>
            <tr>
              <td>6</td>
              <td>h(6) = 1 + 5*6 = 31</td>
              <td>40</td>
              <td>-9</td>
              <td>81</td>
            </tr>
            <tr>
              <td>7</td>
              <td>h(7) = 1 + 5*7 = 36</td>
              <td>50</td>
              <td>-14</td>
              <td>196</td>
            </tr>
            <tr>
              <td>8</td>
              <td>h(8) = 1 + 5*8 = 41</td>
              <td>60</td>
              <td>-19</td>
              <td>361</td>
            </tr>
            <tr>
              <td>9</td>
              <td>H(9) = 1 + 5*9 = 46</td>
              <td>70</td>
              <td>-24</td>
              <td>576</td>
            </tr>
            <tr>
              <td colspan="4"><strong>Sum of squared differences</strong></td>
              <td>1230</td>
            </tr>
            <tr>
              <td colspan="4" style="background: #EEE;"><strong>Cost function J(1, 5)</strong></td>
              <td style="background: #EEE;"><strong>123</strong></td>
            </tr>
          </table>

          <br/>

          <figure>Table 2: Calculating the cost function for h(x) = 3 + 11x</figure>
          <table>
            <thead>
            <tr>
              <td>x</td>
              <td>Predicted Value</td>
              <td>Observed Value</td>
              <td>Difference</td>
              <td>Squared Difference</td>
            </tr>
          </thead>
            <tr>
              <td>5</td>
              <td>h(5) = 3 + 11*5 = 58</td>
              <td>30</td>
              <td>-28</td>
              <td>784</td>
            </tr>
            <tr>
              <td>6</td>
              <td>h(6) = 3 + 11*6 = 69</td>
              <td>40</td>
              <td>-29</td>
              <td>841</td>
            </tr>
            <tr>
              <td>7</td>
              <td>h(7) = 3 + 11*7 = 80</td>
              <td>50</td>
              <td>-30</td>
              <td>900</td>
            </tr>
            <tr>
              <td>8</td>
              <td>h(8) = 3 + 11*8 = 91</td>
              <td>60</td>
              <td>-31</td>
              <td>961</td>
            </tr>
            <tr>
              <td>9</td>
              <td>h(9) = 3 + 11*9 = 102</td>
              <td>70</td>
              <td>-32</td>
              <td>1024</td>
            </tr>
            <tr>
              <td colspan="4"><strong>Sum of squared differences</strong></td>
              <td>4510</td>
            </tr>
            <tr>
              <td colspan="4" style="background: #EEE;"><strong>Cost function J(3, 11)</strong></td>
              <td style="background: #EEE;"><strong>451</strong></td>
            </tr>
          </table>

          <p>
            By going through this exercise, we've been able to objectively demonstrate that the hypothesis function <strong>h(x) = 1 + 5x</strong> with a
            cost of 123 is in fact a quantifiably better predictor for this training set than the hypothesis function <strong>h(x) = 3 + 11x</strong> with a cost of 451.
          </p>

          <p>
            As an exercise for the reader, come up with values for Θ<sub>0</sub> and Θ<sub>1</sub> that would produce an even better predictor for this training set
            then J(1, 5) and prove it by calculating the cost.
          </p>

          <p>
            The hypothesis function is used to model data, and the cost function is used to measure a model’s performance. In the final step of our journey,
            we’ll introduce the gradient descent algorithm – a data-driven, algorithmic method to identify the <em>optimal model</em> for a training set.
          </p>


          <h2>The Gradient Descent Algorithm</h2>

          <p>
            The <strong>gradient descent algorithm</strong> has broad applicability in machine learning as it offers an efficient means to <em>minimize functions</em>. Linear regression uses gradient descent to
            minimize the cost function <strong>J(Θ<sub>0</sub>,Θ<sub>1</sub>)</strong> and discovers optimal values Θ<sub>0</sub> and Θ<sub>1</sub>. Before we define the gradient descent algorithm, let’s gain an intuitive
            understanding of how the algorithm works and an appreciation of its limitations.
          </p>


          <h3>The Gradient Descent Hiking Analogy</h3>
          <p>
            The gradient descent algorithm is very efficient at discovering <strong>local minima</strong> - the smallest value of a function in and around
            a <em>local area</em> of a function. Discovering the local minima of a function is what ultimately yields us with the optimal values
            for Θ<sub>0</sub> and Θ<sub>1</sub> - in other words, the local minima of a function represents the <em>solution</em> for linear regression.
         </p>

          <p>
            Figure 6 below will help you get a sense of what local minima are, and how the gradient descent algorithm
            works to discover them.
          </p>

          <figure>Figure 6: The gradient descent algorithm - discovering minima by intuition</figure>
          <img src="images/chapter2/figure6.png"></img>

          <p>
            Let’s assume that the function in figure 6 represents a side-on view of a mountain range; the green star represents your current location on
            the mountain. You notice a storm rolling in, and visibility is getting poor – to ride out the storm in safety you quickly begin making your
            way down to the base of the mountain. Let’s describe an approach to descend the mountain as a series of simple, logical steps:
          </p>

          <ol>
            <li>You look around, and find a new location that is lower than your current location.</li>
            <li>You take a number of steps in the direction of the new location.</li>
            <li>Once you reach your new location, you repeat steps 1 and 2 until you can no longer find a location that is any lower. To the best of your knowledge, you’ve reached the base of the mountain, represented by the green circle in figure 6.</li>
          </ol>

          <p>
            These three steps provide an intuitive representation of how the gradient descent algorithm works in practice. The green circle in figure 6
            represents one of six local minima in the function – it’s the lowest point within a localized area of the function.
          </p>

          <h3>The Limitations of Gradient Descent</h3>
          <p>
            Had we started this same exercise from the blue star, we would have discovered an even smaller value, and consequently a more optimal solution than
            we did starting from the green star.
          </p>

          <p>
            The conclusion we can draw from this exercise is that the gradient descent algorithm is not well suited for minimizing functions that have more than
            one local minima, since the result is highly dependent on your initial starting position. You would never be certain if the algorithm found the
            <em>optimal</em> values for Θ<sub>0</sub> and Θ<sub>1</sub> if the algorithm is run against functions with multiple local minima. Consequently, the gradient descent
            algorithm should only be used to minimize functions that have a single, global minima. The linear regression cost function <em>does</em> have a single global
            minima, so it’s well suited to be minimized by the gradient descent algorithm.
          </p>

          <h3>Visualizing the Topology of the Cost Function</h3>
          <p>
            The reasoning behind why linear regression cost functions produce a single global minima goes a bit beyond the scope of linear regression. However,
            we are going to present a visualization of the cost function so you can appreciate the “terrain” that the gradient descent algorithm will traverse,
            and provide you with some insight into what the visualization represents with a thought experiment.
          </p>
          <p>
            In our introduction to the cost function, we went through a practical exercise to compute the cost of our hypothesis function with arbitrary values
            for Θ<sub>0</sub> and Θ<sub>1</sub>. The results that we computed were as follows:
          </p>
          <p>
            J(1, 5) = 123 <br/>
            J(3, 11) = 451
          </p>

          <p>
            Let’s imagine plotting these two results on a graph - since we have three variables to plot (Θ<sub>0</sub>, Θ<sub>1</sub> and the cost), we need to plot this data on a 3D
            graph. We place Θ<sub>0</sub> on the x axis, Θ<sub>1</sub> on the y axis, and the cost on the z axis – what we end up with is a visualization of cost <em>as a function of</em>
            Θ<sub>0</sub> and Θ<sub>1</sub>. At this stage, the graph would consist 2 distinct points sitting in a 3D space.
          </p>

          <p>
            Now, let’s assume that we have a lot of free time on our hands, and compute the cost function thousands of times, each with slightly different values
            of Θ<sub>0</sub> and Θ<sub>1</sub> and plot these points on our 3D graph. As you plot this data, a distinct shape will begin to emerge; computing and plotting an
            arbitrarily large number of these data points will inevitably reveal the concave structure in figure 7.
          </p>

          <figure>Figure 7: J(Θ<sub>0</sub>, Θ<sub>1</sub>) as a function of Θ<sub>0</sub> and Θ<sub>1</sub></figure>
          <img src="images/chapter2/figure7.png"></img>

          <p>
            This graph presents cost J(Θ<sub>0</sub>, Θ<sub>1</sub>) as a function of Θ<sub>0</sub> and Θ<sub>1</sub>, and accurately represents the topology that
            our gradient descent algorithm will navigate. All linear regression cost functions will end up producing this concave shape, which is why we can use the
            gradient descent algorithm to minimize linear regression cost functions.
          </p>

          <p>
            Areas shown in red represent high costs, and are consequently <em>bad predictors</em>. Areas shown in blue represent low costs, and are
            consequently <em>good predictors</em>. The gradient descent algorithm will discover the local minima of the function, which will ultimately
            discover the lowest cost and the best predictor – in other words, the optimal values for Θ<sub>0</sub> and Θ<sub>1</sub>.
          </p>

          <h3>The Gradient Descent Algorithm</h3>

          <p>
            So far we’ve described the gradient descent algorithm intuitively, as three simple steps to traverse down a mountain. We’ll now formalize
            this intuition by introducing the gradient descent algorithm:
          </p>

          <p>repeat until convergence, for j = 0 and j = 1: { </p>
          <div style="padding-left: 50px;">
            $$ \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) $$
          </div>
          <p style="margin-top: -15px;">}</p>

          <p>
            While this definition may initially seem unintuitive, we’ll work to demystify the gradient descent algorithm to show that it’s
            simply a formal representation of our earlier intuition. Figure 8 below presents the gradient descent algorithm overlayed
            with the concepts and intuition in our mountain hiking analogy; this should help provide some context around how the algorithm works in practice.
          </p>

          <figure>Figure 8: The gradient descent algorithm with an overlay of the mountain analogy</figure>
          <img src="images/chapter2/figure8.png"></img>

          <p>Let’s review some of the new terms introduced in the gradient descent algorithm:</p>

          <table>
            <tr>
              <td>$$ := $$</td>
              <td><p style="margin: 0; padding: 0;">The expression :=  means <em>assignment</em>. The assignment ensures that we don’t lose reference to our current location as the algorithm iteratively converges towards the right solution.</p></td>
            </tr>
            <tr>
              <td>$$ \alpha $$</td>
              <td><p style="margin: 0; padding: 0;">This is the term <em>alpha</em>, also known as the <em>learning rate</em> and defines the size of the step you want to take. We will discuss the learning rate in detail shortly.</p></td>
            </tr>
            <tr>
              <td>$$ \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) $$</td>
              <td><p style="margin: 0; padding: 0;">This term represents the <em>derivative</em> of the cost function and defines the direction of the steps you’re taking.  There’s a bit more to unpack here, and we’ll provide more insight about how this term works shortly.</p></td>
            </tr>
          </table>

          <p>
            The algorithm refers to repeating until convergence, for j = 0 and j = 1. What does this statement mean?
          </p>

          <p>
            Put simply, when you take a step you’re moving some distance along the x axis (Θ<sub>0</sub>) and some distance along the y axis (Θ<sub>0</sub>).
            The gradient descent algorithm needs a practical means to <em>capture</em> these x y coordinates so that the algorithm has a reference to where it currently
            is, and where it needs to move.
          </p>

          <p>
            The statement for <em>j = 0 and j = 1</em> is simply a <em>shorthand notation</em> used by the gradient descent algorithm to express the fact that
            we need to capture and store these two distinct variables. Let’s express the definition of the gradient descent algorithm in a way that explicitly
            tracks and records the x and y coordinates:
          </p>

          <p>repeat until convergence: { </p>
          <div style="padding-left: 50px;">
          $$ \theta_0 := \theta_0 - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) $$
          $$ \theta_1 := \theta_1 - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) $$
          </div>
          <p style="margin-top: -15px;">}</p>

          <p>
            Let’s substitute the cost function term J(Θ<sub>0</sub>, Θ<sub>1</sub>) with the actual cost function that we defined in the previous section.
            The gradient descent algorithm can now be expressed as:
          </p>

          <p>repeat until convergence: {</p>
          <div style="padding-left: 50px;">
            $$ \theta_0 := \theta_0 - \alpha \frac{\partial}{\partial \theta_0} \dfrac {1}{2m} \displaystyle \sum _{i=0}^m \left (\theta_0 + \theta_1x^{(i)} - y^{(i)} \right)^2 $$
            $$ \theta_1 := \theta_1 - \alpha \frac{\partial}{\partial \theta_1} \dfrac {1}{2m} \displaystyle \sum _{i=0}^m \left (\theta_0 + \theta_1x^{(i)} - y^{(i)} \right)^2$$
          </div>
          <p style="margin-top: -15px;">}</p>

          <p>
            The last step in the development of our algorithm requires us to take partial derivatives of these functions, which requires some knowledge of
            multivariate calculus. While you are more than welcome to work through the derivations yourself, we’ve worked through the derivations for you.
          </p>

          <p>repeat until convergence: { </p>
          <div style="padding-left: 50px;">
            $$ \theta_0 := \theta_0 - \alpha \dfrac {1}{m} \displaystyle \sum _{i=0}^m \left (\theta_0 + \theta_1x^{(i)} - y^{(i)} \right) $$
            $$ \theta_1 := \theta_1 - \alpha \dfrac {1}{m} \displaystyle \sum _{i=0}^m \left (\theta_0 + \theta_1x^{(i)} - y^{(i)} \right)x^{(i)} $$
          </div>
          <p style="margin-top: -15px;">}</p>

          <p>
            It took a bit of effort to get here, but we’ve successfully developed our first machine learning algorithm! Implementing this algorithm will allow
            you to predict the price of a home in Richmond far more accurately than our original best guess prediction that we made at the beginning of this chapter.
          </p>

          <p>
            To close out our introduction to linear regression, we’ll describe convergence in gradient descent and learn how to manage <em>alpha</em>, the learning rate. For readers with a
            programming background, these will be the last two pieces of information that you’ll need to implement this algorithm in software.
          </p>

          <h3>Convergence in Gradient Descent</h3>
          <p>
            In the definition of the gradient descent algorithm, we start with the statement <em>repeat until convergence</em>. In this section, we’ll define what
            convergence means and provide you with some insight into how the derivative term in the gradient descent algorithm helps converge on an optimum solution.
          </p>

          <div class="row mt-5">
            <div class="col-md-6 text-right pr-2"><a href="chapter01.html">&laquo; Chapter 1</a> &nbsp;&bull;</div>
            <div class="col-md-6 text-left pl-0"><a href="chapter03.html">Chapter 3 &raquo;</a></div>
          </div>
</html>
